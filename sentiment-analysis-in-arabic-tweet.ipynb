{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis in Arabic tweets\n\n### **Introduction**\n\n* **Natural Language Processing (NLP):** The discipline of computer science, artificial intelligence, and linguistics that is concerned with the creation of computational models that process and understand natural language. These include: making the computer understand the semantic grouping of words (e.g. cat and dog are semantically more similar than cat and spoon), text to speech, language translation and, many more\n\n* **Sentiment Analysis:** It is the interpretation and classification of emotions (positive, negative, and neutral) within text data using text analysis techniques. Sentiment analysis allows organizations to identify public sentiment towards certain words or topics.\n\n![image](https://drive.google.com/uc?export=view&id=1KQkdq_eJ1dqIEnOIUdR3Gvi8uRe9pQqi)","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n1. [Environment Setup](#p1)\n    - [Enable the GPU](#p1.1)\n    - [Dependencies Installation](#p1.2)\n2. [Dataset Importing](#p2)\n3. [Dataset Preparation](#p3)\n    - [Dataset Cleaning](#p3.1)\n    - [Dataset Tokenization](#p3.2)\n    - [Label Encoding](#p3.3)\n    - [Train Test Spliting](#p3.4)\n4. [Build Classical Machine learning Models](#p4)\n    - [TF-IDF Embedding](#p4.1)\n    - [Train Different Classifiers and Select the Champion Model](#p4.2)\n5. [Tramsfer Learning a Pre-trained Models](#p5)\n6. [Infer the Test Data and Prepart the Submission File](#p6)","metadata":{}},{"cell_type":"markdown","source":"# <a name=\"p1\">Environment Setup</a>","metadata":{}},{"cell_type":"markdown","source":"### <a name=\"p1.1\">Enable the GPU</a>","metadata":{}},{"cell_type":"code","source":"import torch\n# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    !nvidia-smi\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-08-23T04:00:16.379843Z","iopub.execute_input":"2022-08-23T04:00:16.380442Z","iopub.status.idle":"2022-08-23T04:00:17.411668Z","shell.execute_reply.started":"2022-08-23T04:00:16.380408Z","shell.execute_reply":"2022-08-23T04:00:17.410378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a name=\"p1.2\">Dependencies Installation</a>","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n!pip install pyarabic\n!pip install farasapy\n!pip install emoji\n!pip install transformers\n!git clone https://github.com/aub-mind/arabert.git","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:44:03.932884Z","iopub.execute_input":"2022-08-23T03:44:03.937946Z","iopub.status.idle":"2022-08-23T03:45:09.748064Z","shell.execute_reply.started":"2022-08-23T03:44:03.937902Z","shell.execute_reply":"2022-08-23T03:45:09.746837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"p2\">Dataset Importing</a>\nThe dataset has been scraped from Twitter and then labeled and used in a local competition in EGPYT, It contains 2,746 tweets extracted using the **Twitter API**. The tweets have been annotated (neg = negative, pos = positive, and neu = neutral) and they can be used to detect sentiment.\n\n**Dataset files:**\n- train.csv - the training set has 2059 unique entry\n- test.csv - the test set has 687 unique entry\n\n\n**The dataset has 2 fields:**\n1. **Tweet**: the text of the tweet\n2. **Class**: the polarity of the tweet **(neg = negative, pos = positive, and neu = neutral)**\n\n![image](https://drive.google.com/uc?export=view&id=1f2RlQTR5QxLbOG4ygTeQZqeMjzZiTJV5)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:45:09.750251Z","iopub.execute_input":"2022-08-23T03:45:09.750656Z","iopub.status.idle":"2022-08-23T03:45:09.761854Z","shell.execute_reply.started":"2022-08-23T03:45:09.750615Z","shell.execute_reply":"2022-08-23T03:45:09.760629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Data_set = pd.read_csv(\"/kaggle/input/nlp-arabic-tweets/train.csv\")\nData_set","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:38:13.784202Z","iopub.execute_input":"2022-08-23T03:38:13.784613Z","iopub.status.idle":"2022-08-23T03:38:13.827020Z","shell.execute_reply.started":"2022-08-23T03:38:13.784552Z","shell.execute_reply":"2022-08-23T03:38:13.826036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"p3\">Dataset Preparation</a>","metadata":{}},{"cell_type":"markdown","source":"### <a name=\"p3.1\">Dataset Cleaning</a>\n- Remove hyperlinks\n- Remove repeated spaces\n- Remove English words\n- Remove mentions\n- Remove emojis\n- Remove tashkeel\n- Remove special charachters\n- Remove repeated letters\n- Remove stop words\n- Apply stemmer\n- Normalize letters","metadata":{}},{"cell_type":"markdown","source":"**Arabic Stop Word File Importing**","metadata":{}},{"cell_type":"code","source":"arabic_stop_words=[]\nwith open ('../input/arabic-helper-filesnlp/Arabic_stop_words.txt',encoding='utf-8') as f :\n    for i in f.readlines() :\n        arabic_stop_words.append(i)\n        arabic_stop_words[-1]=arabic_stop_words[-1][:-1]","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:43:38.215801Z","iopub.execute_input":"2022-08-23T03:43:38.216786Z","iopub.status.idle":"2022-08-23T03:43:38.232114Z","shell.execute_reply.started":"2022-08-23T03:43:38.216741Z","shell.execute_reply":"2022-08-23T03:43:38.231103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The Cleaning Function**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport string,emoji, re\nimport pyarabic.araby as ar\nimport functools, operator\nimport logging\nlogging.basicConfig(level=logging.WARNING)\nlogger = logging.getLogger(__name__)\n\ndef get_emoji_regexp():\n    # Sort emoji by length to make sure multi-character emojis are matched first\n    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n    pattern = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n    return re.compile(pattern)\n\ndef data_cleaning (text):\n    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r\"http\\S+\", \"\", text)\n    text = re.sub(r\"https\\S+\", \"\", text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(\"(\\s\\d+)\",\"\",text)\n    text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n    text = re.sub(\"\\d+\", \" \", text)\n    text = ar.strip_tashkeel(text)\n    text = ar.strip_tatweel(text)\n    text = text.replace(\"#\", \" \");\n    text = text.replace(\"@\", \" \");\n    text = text.replace(\"_\", \" \");\n    \n    # Remove arabic signs\n    text = text[0:2] + ''.join([text[i] for i in range(2, len(text)) if text[i]!=text[i-1] or text[i]!=text[i-2]])\n    text =  re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', text)\n    text =  '' if text in arabic_stop_words else text\n    from nltk.stem.isri import ISRIStemmer\n    text=ISRIStemmer().stem(text)\n    \n    translator = str.maketrans('', '', string.punctuation)\n    text = text.translate(translator)\n    em = text\n    em_split_emoji = get_emoji_regexp().split(em)\n    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n    em_split = functools.reduce(operator.concat, em_split_whitespace)\n    text = \" \".join(em_split)\n    text = re.sub(r'(.)\\1+', r'\\1', text)\n    \n    text = text.replace(\"آ\", \"ا\")\n    text = text.replace(\"إ\", \"ا\")\n    text = text.replace(\"أ\", \"ا\")\n    text = text.replace(\"ؤ\", \"و\")\n    text = text.replace(\"ئ\", \"ي\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:43:39.242831Z","iopub.execute_input":"2022-08-23T03:43:39.243430Z","iopub.status.idle":"2022-08-23T03:43:39.282614Z","shell.execute_reply.started":"2022-08-23T03:43:39.243396Z","shell.execute_reply":"2022-08-23T03:43:39.281629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Apply the Cleaning Function**","metadata":{}},{"cell_type":"code","source":"Data_set['tweet']=Data_set['tweet'].apply(lambda x: data_cleaning(x))\nData_set","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:43:41.392956Z","iopub.execute_input":"2022-08-23T03:43:41.393314Z","iopub.status.idle":"2022-08-23T03:43:51.476030Z","shell.execute_reply.started":"2022-08-23T03:43:41.393283Z","shell.execute_reply":"2022-08-23T03:43:51.474936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a name=\"p3.2\">Dataset Tokenization</a>","metadata":{}},{"cell_type":"code","source":"from arabert.preprocess import ArabertPreprocessor\n\nmodel_name = \"UBC-NLP/MARBERT\"\ndf = Data_set\narabert_prep = ArabertPreprocessor(model_name=model_name)\ndf['tweet']=Data_set['tweet'].apply(lambda x: arabert_prep.preprocess(x))\ndf","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:47:51.841974Z","iopub.execute_input":"2022-08-23T03:47:51.842381Z","iopub.status.idle":"2022-08-23T03:47:52.050825Z","shell.execute_reply.started":"2022-08-23T03:47:51.842349Z","shell.execute_reply":"2022-08-23T03:47:52.049723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a name=\"p3.3\">Label Encoding</a>","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\nlable_encoder = preprocessing.LabelEncoder()\n\nencoded_labels=lable_encoder.fit_transform(Data_set[\"class\"])\ndf['class']=encoded_labels\ndf","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:36:52.216591Z","iopub.execute_input":"2022-08-22T12:36:52.216870Z","iopub.status.idle":"2022-08-22T12:36:52.231490Z","shell.execute_reply.started":"2022-08-22T12:36:52.216839Z","shell.execute_reply":"2022-08-22T12:36:52.230043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a name=\"p3.4\">Train Test Spliting</a>","metadata":{}},{"cell_type":"code","source":"seed = 42\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation=train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T03:51:29.826214Z","iopub.execute_input":"2022-08-23T03:51:29.826615Z","iopub.status.idle":"2022-08-23T03:51:29.836765Z","shell.execute_reply.started":"2022-08-23T03:51:29.826582Z","shell.execute_reply":"2022-08-23T03:51:29.835863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"p4\">Build Classical Machine learning Models</a>\n![image](https://drive.google.com/uc?export=view&id=1IirV4FvQQ9Bd_JZnOMGTc2o7EcF61oY2)","metadata":{}},{"cell_type":"markdown","source":"### <a name=\"p4.1\">TF-IDF Embedding</a>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_ngram(n_gram,X_train,X_val):\n    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n    x_train_vec = vectorizer.fit_transform(X_train)\n    x_test_vec = vectorizer.transform(X_val)\n    return x_train_vec,x_test_vec","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:36:52.243047Z","iopub.execute_input":"2022-08-22T12:36:52.243687Z","iopub.status.idle":"2022-08-22T12:36:52.252728Z","shell.execute_reply.started":"2022-08-22T12:36:52.243651Z","shell.execute_reply":"2022-08-22T12:36:52.251871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying tfidf with 1-gram, and 2-gram\ntfidf_1g_transformation_train,tfidf_1g_transformation_validation= tfidf_ngram(1,X_train,X_validation)\ntfidf_2g_transformation_train,tfidf_2g_transformation_validation= tfidf_ngram(2,X_train,X_validation)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:36:52.253987Z","iopub.execute_input":"2022-08-22T12:36:52.254874Z","iopub.status.idle":"2022-08-22T12:36:52.357820Z","shell.execute_reply.started":"2022-08-22T12:36:52.254838Z","shell.execute_reply":"2022-08-22T12:36:52.356743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <a name=\"p4.2\">Train Different Classifiers and Select the Champion Model</a>","metadata":{}},{"cell_type":"code","source":"%matplotlib ipympl\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\nimport matplotlib.pyplot as plt\n\ntext_embedding={\n    'TF_IDF 1_gram':(tfidf_1g_transformation_train,tfidf_1g_transformation_validation),\n    'TF_IDF 2_gram':(tfidf_2g_transformation_train,tfidf_2g_transformation_validation)\n}\nmodels=[SVC(), KNeighborsClassifier(), XGBClassifier(), RandomForestClassifier(), DecisionTreeClassifier(), LogisticRegression(), MultinomialNB()]\n\n\nhighest_test_accuracy=0\nchampion_model_name=''\nchampion_model=''\nchampion_embedding=''\nresults_dict={'Model Name':[],'Embedding type':[],'Training Accuracy':[],'Testing Accuracy':[]}\n\nfor model in models:\n  for embedding_vector in text_embedding.keys():\n    train=text_embedding[embedding_vector][0]\n    test=text_embedding[embedding_vector][1]\n    model.fit(train, y_train)\n    results_dict['Model Name'].append(type(model).__name__)\n    results_dict['Embedding type'].append(embedding_vector)\n    train_acc=model.score(train, y_train)\n    results_dict['Training Accuracy'].append(train_acc)\n    test_acc=model.score(test, y_validation)\n    results_dict['Testing Accuracy'].append(test_acc)\n    if test_acc > highest_test_accuracy:\n      highest_test_accuracy=test_acc\n      champion_model_name=type(model).__name__\n      champion_model=model\n      champion_embedding=embedding_vector\n\nresults_df=pd.DataFrame(results_dict)\nresults_df['Model Name']=results_df['Model Name'].apply(lambda x: x[:-10] if 'Classifier' in x else x)\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:36:52.359479Z","iopub.execute_input":"2022-08-22T12:36:52.359924Z","iopub.status.idle":"2022-08-22T12:37:06.846394Z","shell.execute_reply.started":"2022-08-22T12:36:52.359860Z","shell.execute_reply":"2022-08-22T12:37:06.845116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('champion_model is ',champion_model_name)\nprint('champion_embedding is',champion_embedding)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:37:06.852704Z","iopub.execute_input":"2022-08-22T12:37:06.855802Z","iopub.status.idle":"2022-08-22T12:37:06.867781Z","shell.execute_reply.started":"2022-08-22T12:37:06.855741Z","shell.execute_reply":"2022-08-22T12:37:06.866343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"p5\">Transfer Learning a Pre-trained Models</a>\n- Used a transformer-based model pre-trained on Arabic language dataset from hugging face called **'MARBERT'** model, chech the link for more Infos [hugging face](https://huggingface.co/UBC-NLP/MARBERT).\n\n![image](https://drive.google.com/uc?export=view&id=1iiQtLmtmTgWAqPsOie8kVXCs65RgJJeT)\n\nKindly, check this link to read more about transfer learning [what-is-transfer-learning-and-why-is-it-needed](https://www.educative.io/answers/what-is-transfer-learning-and-why-is-it-needed)","metadata":{}},{"cell_type":"markdown","source":"**Model and Tokenizer initialization**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ntokenizer =AutoTokenizer.from_pretrained('UBC-NLP/MARBERT')\nmodel = AutoModelForSequenceClassification.from_pretrained('UBC-NLP/MARBERT', num_labels=3)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:48:58.279964Z","iopub.execute_input":"2022-08-22T12:48:58.280653Z","iopub.status.idle":"2022-08-22T12:49:04.860881Z","shell.execute_reply.started":"2022-08-22T12:48:58.280610Z","shell.execute_reply":"2022-08-22T12:49:04.859950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize the sentences using bert tokenizer\ndf[\"bert_tokens\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"bert_tokens_ids\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\ndf[\"encoded\"] = df.tweet.apply(lambda x: tokenizer.encode_plus(x,return_tensors='pt')['input_ids'])\ndf","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:04.862876Z","iopub.execute_input":"2022-08-22T12:49:04.863333Z","iopub.status.idle":"2022-08-22T12:49:05.647695Z","shell.execute_reply.started":"2022-08-22T12:49:04.863288Z","shell.execute_reply":"2022-08-22T12:49:05.646549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyper-parameters**","metadata":{}},{"cell_type":"code","source":"# Number of training epochs\nepochs = 20\n# Select the max sentance lenth\nMAX_LEN = 80\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size =64","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.649231Z","iopub.execute_input":"2022-08-22T12:49:05.649859Z","iopub.status.idle":"2022-08-22T12:49:05.655435Z","shell.execute_reply.started":"2022-08-22T12:49:05.649818Z","shell.execute_reply":"2022-08-22T12:49:05.653540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Padding**\n- Set the maximum sequence length. The longest sequence in our training set is 39, but we'll leave room at the end anyway.\n- Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n- Apply pad_sequences on our input tokens","metadata":{}},{"cell_type":"code","source":"from keras_preprocessing.sequence import pad_sequences\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in df['bert_tokens']]\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.658665Z","iopub.execute_input":"2022-08-22T12:49:05.659038Z","iopub.status.idle":"2022-08-22T12:49:05.701495Z","shell.execute_reply.started":"2022-08-22T12:49:05.659003Z","shell.execute_reply":"2022-08-22T12:49:05.700628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Attention mask**\n- Create the attention mask list\n- Create a mask of 1s for each token followed by 0s for padding","metadata":{}},{"cell_type":"code","source":"attention_masks = []\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.703495Z","iopub.execute_input":"2022-08-22T12:49:05.704132Z","iopub.status.idle":"2022-08-22T12:49:05.778803Z","shell.execute_reply.started":"2022-08-22T12:49:05.704098Z","shell.execute_reply":"2022-08-22T12:49:05.777964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build the DataLoader**\n- Convert all of our data into torch tensors, the required datatype for our model\n- Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory\n\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\n# Use train_test_split to split our data into train and validation sets for training\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, encoded_labels,\n                                                             random_state=seed, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                                             random_state=seed, test_size=0.1)\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_dataloader = DataLoader(validation_data, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.780206Z","iopub.execute_input":"2022-08-22T12:49:05.780541Z","iopub.status.idle":"2022-08-22T12:49:05.801623Z","shell.execute_reply.started":"2022-08-22T12:49:05.780509Z","shell.execute_reply":"2022-08-22T12:49:05.800803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Optimizer parameters**","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n\n# This variable contains all of the hyperparemeter information our training loop needs\noptimizer = optim.AdamW(optimizer_grouped_parameters,lr=.000005)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.802866Z","iopub.execute_input":"2022-08-22T12:49:05.803283Z","iopub.status.idle":"2022-08-22T12:49:05.812061Z","shell.execute_reply.started":"2022-08-22T12:49:05.803244Z","shell.execute_reply":"2022-08-22T12:49:05.811114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Transfer learning on our dataset**","metadata":{}},{"cell_type":"code","source":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.813471Z","iopub.execute_input":"2022-08-22T12:49:05.814315Z","iopub.status.idle":"2022-08-22T12:49:05.824385Z","shell.execute_reply.started":"2022-08-22T12:49:05.814279Z","shell.execute_reply":"2022-08-22T12:49:05.823459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import trange\nimport numpy as np\n\nt = []\n# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\nif torch.cuda.is_available():\n    # Transfer the model to GPU\n    model.to(\"cuda\")\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n  # Training\n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train()\n  # Tracking variables\n  tr_loss = 0\n  nb_tr_examples, nb_tr_steps = 0, 0\n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):\n    # Add batch to GPU\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n    # Forward pass\n    if torch.cuda.is_available():\n        loss = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"), labels=b_labels.to(\"cuda\"))[\"loss\"]\n    else:\n        loss = model(b_input_ids.to(\"cpu\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cpu\"), labels=b_labels.to(\"cpu\"))[\"loss\"]\n\n    train_loss_set.append(loss.item())\n    # Backward pass\n    loss.backward()\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n\n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n  # Validation\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n  # Tracking variables\n  eval_loss, eval_accuracy = 0, 0\n  nb_eval_steps, nb_eval_examples = 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    # batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n        if torch.cuda.is_available():\n            logits = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"))\n        else:\n            logits = model(b_input_ids.to(\"cpu\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cpu\"))\n\n    # Move logits and labels to CPU\n    logits = logits[\"logits\"].detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n    eval_accuracy += tmp_eval_accuracy\n    nb_eval_steps += 1\n\n  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n  if (eval_accuracy/nb_eval_steps) > 0.78:\n    break","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:49:05.826257Z","iopub.execute_input":"2022-08-22T12:49:05.826763Z","iopub.status.idle":"2022-08-22T12:50:45.275350Z","shell.execute_reply.started":"2022-08-22T12:49:05.826724Z","shell.execute_reply":"2022-08-22T12:50:45.274174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <a name=\"p6\">Infer the Test Data and Prepare the Submission File</a>\n\n![image](https://drive.google.com/uc?export=view&id=1o72wCZZQIF5DcGlzUBK9yYIkiqlwHrT-)","metadata":{}},{"cell_type":"markdown","source":"#### **Testing Dataset Preperation**","metadata":{}},{"cell_type":"code","source":"df_submit = pd.read_csv(\"../input/nlp-arabic-tweets/test.csv\")\ndf_submit[\"tweet\"] = df_submit.tweet.apply(lambda x: data_cleaning(x))\ndf_submit['tweet']=df_submit['tweet'].apply(lambda x: arabert_prep.preprocess(x))\n# Tokenize the sentences using bert tokenizer\ndf_submit[\"bert_tokens\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:50:59.374223Z","iopub.execute_input":"2022-08-22T12:50:59.374787Z","iopub.status.idle":"2022-08-22T12:51:02.324409Z","shell.execute_reply.started":"2022-08-22T12:50:59.374750Z","shell.execute_reply":"2022-08-22T12:51:02.323437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in df_submit[\"bert_tokens\"]]\ninput_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\nattention_masks_submit = []\nfor seq in input_ids_submit:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks_submit.append(seq_mask)\n    \ninputs_submit = torch.tensor(input_ids_submit)\nmasks_submit = torch.tensor(attention_masks_submit)\nsubmit_data = TensorDataset(inputs_submit, masks_submit)\nsubmit_dataloader = DataLoader(submit_data, batch_size=batch_size)\nmodel.eval()\nif torch.cuda.is_available():\n    model.to(\"cuda\")\n\noutputs = []\nfor input, masks in submit_dataloader:\n    torch.cuda.empty_cache() # empty the gpu memory\n    if torch.cuda.is_available():\n        # Transfer the batch to gpu\n        input = input.to('cuda')\n        masks = masks.to('cuda')\n    # Run inference on the batch\n    output = model(input, attention_mask=masks)[\"logits\"]\n    # Transfer the output to CPU again and convert to numpy\n    output = output.cpu().detach().numpy()\n    # Store the output in a list\n    outputs.append(output)\n# Concatenate all the lists within the list into one list\noutputs = [x for y in outputs for x in y]\n# Inverse transform the label encoding\npred_flat = np.argmax(outputs, axis=1).flatten()\noutput_labels = lable_encoder.inverse_transform(pred_flat)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:51:02.326313Z","iopub.execute_input":"2022-08-22T12:51:02.326726Z","iopub.status.idle":"2022-08-22T12:51:04.992126Z","shell.execute_reply.started":"2022-08-22T12:51:02.326690Z","shell.execute_reply":"2022-08-22T12:51:04.991115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Create the Submission File**\n![image](https://drive.google.com/uc?export=view&id=19ryQs1gNHFYqiYtqbnAQ8RFhpf7uPRMa)\n","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n# save (submission)\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-22T12:51:04.993902Z","iopub.execute_input":"2022-08-22T12:51:04.994253Z","iopub.status.idle":"2022-08-22T12:51:05.004400Z","shell.execute_reply.started":"2022-08-22T12:51:04.994214Z","shell.execute_reply":"2022-08-22T12:51:05.003413Z"},"trusted":true},"execution_count":null,"outputs":[]}]}